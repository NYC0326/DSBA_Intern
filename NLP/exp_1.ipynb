{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "torch.cuda.set_per_process_memory_fraction(11/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(11/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드\n",
    "train/test 로 구성되어 있는 것을 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = load_dataset('imdb')\n",
    "imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation을 위한 데이터 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid = imdb['train']\n",
    "test = imdb['test']\n",
    "# split train/validation\n",
    "train_valid = train_valid.train_test_split(test_size=0.1)\n",
    "train = train_valid['train']\n",
    "valid = train_valid['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label distribution for train dataset : [11260, 11240]\n"
     ]
    }
   ],
   "source": [
    "pos_neg_count = [0, 0]\n",
    "for example in train:\n",
    "    pos_neg_count[example['label']] += 1\n",
    "print(f\"label distribution for train dataset : {pos_neg_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label distribution for train dataset : [1240, 1260]\n"
     ]
    }
   ],
   "source": [
    "pos_neg_count = [0, 0]\n",
    "for example in valid:\n",
    "    pos_neg_count[example['label']] += 1\n",
    "print(f\"label distribution for train dataset : {pos_neg_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample text : I first saw this absolutely riveting documentary in it's initial release back in 2001,and it really had a profound effect on me, so much that I bugged several of my friends to see it with me on repeat screenings. The bottom line:none of my friends walked away disappointed (ever!). This stellar film is about Scottish conceptual artist, Andy Goldsworthy,who creates some absolutely beautiful pieces of art using natural materials (wood,water,flowers,rocks,etc.)to create pieces that eventually return to their natural form (a statement in the temporary state of everything?). We get to see Goldsworthy create several works of temporary art,as well as some of his long term installations in major galleries around the world,as well as a few pieces in the natural world,as well. German film maker,Thomas Riedelsheimer directs,photographs & edits this meditation on the creative process that is a real treat for both the eye & ear (with an ambient musical score,composed & performed by Fred Frith,who's music is generally edgy experimental/noise textured guitar,as well as a capable ensemble of musicians). Although this film has been available on DVD for some years now,if you can find a cinema that is highlighting a revival of this fine film,by all means,seek it out (it's easily a film that was composed for the large screen,with a proficient sound system to truly experience this film the right way). No MPAA rating,but contains nothing to offend (unless the live birth of a sheep on screen is destined to offend or disturb)\n",
      "train sample label : 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"train sample text : {train[0]['text']}\")\n",
    "print(f\"train sample label : {train[0]['label']}\") # 0: negative, 1: positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 단순 Model, Tokenizer 로드\n",
    "Pretrained LM을 사용할 때는 함께 학습된 tokenizer를 이용해야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 02:48:29.145984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\", add_pooling_layer=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP에서 모델 입력은 다음 순서로 이루어집니다. \n",
    "1. 원본 텍스트\n",
    "2. Tokenized Input : Tokenizer를 통과하여 모델의 입력으로 전처리된 데이터\n",
    "3. Embedding or Contextulaized Vector or Hidden Representation : 모델을 통과하여 생성된 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : I first sa\n",
      "tokenized text : ['i', 'first', 'sa']\n",
      "tokenized text with special tokens : ['[CLS]', 'i', 'first', 'sa', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "input_text = train[0]['text'][:10]\n",
    "tokenized_text = tokenizer.tokenize(input_text)\n",
    "tokenized_text_with_special_tokens = tokenizer.tokenize(input_text, add_special_tokens=True)\n",
    "print(f\"original text : {input_text}\")\n",
    "print(f\"tokenized text : {tokenized_text}\")\n",
    "print(f\"tokenized text with special tokens : {tokenized_text_with_special_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : tensor([ 101, 1045, 2034, 7842,  102])\n",
      "token_type_ids : tensor([0, 0, 0, 0, 0])\n",
      "attention_mask : tensor([1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input_text = train[0]['text'][:10]\n",
    "tokenized_input = tokenizer(input_text, return_tensors='pt')\n",
    "for key, value in tokenized_input.items():\n",
    "    print(f\"{key} : {value[0]}\")\n",
    "\n",
    "# input_ids : 각 토큰의 vocab id(embedding layer의 해당 토큰 vector와 맵핑)\n",
    "# token_type_ids : segment embedding을 위한 id (BERT 등 특정 모델에서만 사용)\n",
    "# attention_mask : attention을 위한 mask (Transformer 기반 모델들은 공통적으로 사용, 사실상 NLP 모든 모델에서 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized text : [101, 1045, 2034, 7842, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] i first sa [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_txt = tokenizer(input_text)['input_ids']\n",
    "print(f\"tokenized text : {tokenized_txt}\")\n",
    "tokenizer.decode(tokenized_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력된 데이터는 token_id -> embedding -> transformer blocks -> representation으로 산출됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. From Text to Logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(train[0]['text'], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1989, -0.0048,  0.2971,  ..., -0.2877,  0.5979,  0.7716],\n",
       "         [ 0.0737, -0.1801, -0.1338,  ...,  0.0100,  0.6722,  0.2568],\n",
       "         [-0.2193, -0.5881, -0.6665,  ...,  0.1343,  0.1884, -0.0985],\n",
       "         ...,\n",
       "         [ 0.4321,  0.1643, -0.2640,  ..., -0.1180,  0.5758, -0.2244],\n",
       "         [-0.3143, -0.6689,  0.0094,  ...,  0.4401,  0.3329, -0.6912],\n",
       "         [-0.0704,  0.6689, -0.1986,  ...,  0.3935,  0.3162,  0.0596]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation = model(**tokenized_input) # 모델 입력으로 input_ids, token_type_ids, attention_mask를 받습니다.\n",
    "representation # 실제로는 NLP 모델들은 여러가지 output을 받아볼 수 있도록 설정되어 있어, 필요한 경우 중간 layer representation, attention map 등도 간단히 return이 가능합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 375, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation['last_hidden_state'].shape # (batch_size, sequence_length, hidden_size)\n",
    "# 각 토큰의 representation이 산출되는 것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하지만 우리가 원하는 것은 하나의 입력 sample에 대한 긍부정 분류 결과일 겁니다. \n",
    "## 이를 위해 linear layer를 통과시켜 logit값을 만들어봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled Output for a Sample : torch.Size([1, 768])\n",
      "Logit for a Sample : torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "classification_head = torch.nn.Linear(768, 2) # bert hidden size -> binary classification\n",
    "pooled_output = torch.mean(representation['last_hidden_state'], dim=1) # 입력 토큰의 모든 representation의 평균을 취합니다.\n",
    "print(f\"Pooled Output for a Sample : {pooled_output.shape}\")\n",
    "logits = classification_head(pooled_output)\n",
    "print(f\"Logit for a Sample : {logits.shape}\") # binary classification을 위한 logit이 산출됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loss Calculation and BackProp for Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for a Sample : 0.9723962545394897\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, torch.tensor([train[0]['label']]))\n",
    "print(f\"Loss for a Sample : {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.word_embeddings.weight.grad.shape # (vocab_size, hidden_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0612,  0.1357,  0.1675,  ..., -0.0110,  0.2155, -0.0380],\n",
       "        [ 0.0612, -0.1357, -0.1675,  ...,  0.0110, -0.2155,  0.0380]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_head.weight.grad.shape # (num_labels, hidden_size)\n",
    "classification_head.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for Attn Value Linear Layer\n",
      "Shape : torch.Size([768, 768])\n",
      "Parameter containing:\n",
      "tensor([[ 1.1403e-02,  1.1445e-03, -9.5199e-03,  ...,  2.3602e-02,\n",
      "         -2.7326e-02,  9.3509e-05],\n",
      "        [-2.6590e-02, -1.2196e-02, -4.2571e-02,  ...,  5.9239e-02,\n",
      "         -1.1393e-02, -2.5042e-02],\n",
      "        [ 1.9075e-02, -2.3710e-02,  3.1961e-02,  ...,  5.6482e-03,\n",
      "         -3.7891e-02, -2.2433e-02],\n",
      "        ...,\n",
      "        [-3.1478e-02, -2.0522e-02,  3.1003e-02,  ..., -3.9800e-02,\n",
      "         -5.0545e-02, -5.9328e-03],\n",
      "        [-6.6795e-03,  4.2564e-02,  4.7859e-02,  ...,  2.5147e-02,\n",
      "          1.6209e-02, -8.9153e-03],\n",
      "        [-1.9984e-03, -2.4571e-02,  5.6221e-03,  ..., -4.9603e-02,\n",
      "         -5.8003e-02, -3.3787e-02]], requires_grad=True)\n",
      "Gradient for Attn Value Linear Layer\n",
      "Shape : torch.Size([768, 768])\n",
      "tensor([[ 1.3102e-04, -1.4789e-03, -9.4285e-04,  ...,  4.8157e-04,\n",
      "         -5.4509e-04,  3.9943e-05],\n",
      "        [-1.6418e-04,  1.2516e-04,  2.0550e-04,  ...,  2.5226e-05,\n",
      "         -3.5675e-05,  1.2847e-04],\n",
      "        [ 3.3830e-04, -7.7994e-05, -2.4192e-04,  ..., -5.7840e-05,\n",
      "          9.3345e-05, -3.0801e-04],\n",
      "        ...,\n",
      "        [-8.1406e-04,  1.4871e-04,  5.3082e-04,  ..., -7.8457e-04,\n",
      "         -4.4532e-04, -9.8431e-05],\n",
      "        [-3.7606e-04, -2.4128e-04, -6.0386e-05,  ..., -3.2711e-05,\n",
      "          1.9941e-06,  3.1398e-04],\n",
      "        [ 1.8858e-04,  1.0264e-04, -6.3326e-05,  ..., -2.6571e-04,\n",
      "          2.5921e-04, -1.4323e-04]])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 layer의 attention value linear layer의 weight를 확인해보겠습니다.\n",
    "print(\"Weight for Attn Value Linear Layer\") \n",
    "print(f\"Shape : {model.base_model.encoder.layer[0].attention.self.value.weight.shape}\")\n",
    "print(model.base_model.encoder.layer[0].attention.self.value.weight)\n",
    "\n",
    "# loss 역전파를 통해 계산된 gradient 역시 아래와 같습니다.\n",
    "print(\"Gradient for Attn Value Linear Layer\")\n",
    "print(f\"Shape : {model.base_model.encoder.layer[0].attention.self.value.weight.grad.shape}\")\n",
    "print(model.base_model.encoder.layer[0].attention.self.value.weight.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Step을 통해 모델을 업데이트시킵시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for Attn Value Linear Layer\n",
      "Shape : torch.Size([768, 768])\n",
      "Parameter containing:\n",
      "tensor([[ 1.1393e-02,  1.1544e-03, -9.5100e-03,  ...,  2.3592e-02,\n",
      "         -2.7316e-02,  8.4646e-05],\n",
      "        [-2.6581e-02, -1.2205e-02, -4.2581e-02,  ...,  5.9242e-02,\n",
      "         -1.1396e-02, -2.5051e-02],\n",
      "        [ 1.9065e-02, -2.3706e-02,  3.1970e-02,  ...,  5.6574e-03,\n",
      "         -3.7900e-02, -2.2423e-02],\n",
      "        ...,\n",
      "        [-3.1468e-02, -2.0527e-02,  3.0994e-02,  ..., -3.9791e-02,\n",
      "         -5.0536e-02, -5.9266e-03],\n",
      "        [-6.6695e-03,  4.2570e-02,  4.7859e-02,  ...,  2.5155e-02,\n",
      "          1.6215e-02, -8.9213e-03],\n",
      "        [-2.0083e-03, -2.4570e-02,  5.6318e-03,  ..., -4.9595e-02,\n",
      "         -5.8012e-02, -3.3781e-02]], requires_grad=True)\n",
      "Gradient for Attn Value Linear Layer\n",
      "Shape : torch.Size([768, 768])\n",
      "tensor([[ 1.3102e-04, -1.4789e-03, -9.4285e-04,  ...,  4.8157e-04,\n",
      "         -5.4509e-04,  3.9943e-05],\n",
      "        [-1.6418e-04,  1.2516e-04,  2.0550e-04,  ...,  2.5226e-05,\n",
      "         -3.5675e-05,  1.2847e-04],\n",
      "        [ 3.3830e-04, -7.7994e-05, -2.4192e-04,  ..., -5.7840e-05,\n",
      "          9.3345e-05, -3.0801e-04],\n",
      "        ...,\n",
      "        [-8.1406e-04,  1.4871e-04,  5.3082e-04,  ..., -7.8457e-04,\n",
      "         -4.4532e-04, -9.8431e-05],\n",
      "        [-3.7606e-04, -2.4128e-04, -6.0386e-05,  ..., -3.2711e-05,\n",
      "          1.9941e-06,  3.1398e-04],\n",
      "        [ 1.8858e-04,  1.0264e-04, -6.3326e-05,  ..., -2.6571e-04,\n",
      "          2.5921e-04, -1.4323e-04]])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 layer의 attention value linear layer의 weight를 확인해보겠습니다.\n",
    "print(\"Weight for Attn Value Linear Layer\") \n",
    "print(f\"Shape : {model.base_model.encoder.layer[0].attention.self.value.weight.shape}\")\n",
    "print(model.base_model.encoder.layer[0].attention.self.value.weight)\n",
    "\n",
    "# loss 역전파를 통해 계산된 gradient 역시 아래와 같습니다.\n",
    "print(\"Gradient for Attn Value Linear Layer\")\n",
    "print(f\"Shape : {model.base_model.encoder.layer[0].attention.self.value.weight.grad.shape}\")\n",
    "print(model.base_model.encoder.layer[0].attention.self.value.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for Attn Value Linear Layer\n",
      "Shape : torch.Size([768, 768])\n",
      "Parameter containing:\n",
      "tensor([[ 1.1393e-02,  1.1544e-03, -9.5100e-03,  ...,  2.3592e-02,\n",
      "         -2.7316e-02,  8.4646e-05],\n",
      "        [-2.6581e-02, -1.2205e-02, -4.2581e-02,  ...,  5.9242e-02,\n",
      "         -1.1396e-02, -2.5051e-02],\n",
      "        [ 1.9065e-02, -2.3706e-02,  3.1970e-02,  ...,  5.6574e-03,\n",
      "         -3.7900e-02, -2.2423e-02],\n",
      "        ...,\n",
      "        [-3.1468e-02, -2.0527e-02,  3.0994e-02,  ..., -3.9791e-02,\n",
      "         -5.0536e-02, -5.9266e-03],\n",
      "        [-6.6695e-03,  4.2570e-02,  4.7859e-02,  ...,  2.5155e-02,\n",
      "          1.6215e-02, -8.9213e-03],\n",
      "        [-2.0083e-03, -2.4570e-02,  5.6318e-03,  ..., -4.9595e-02,\n",
      "         -5.8012e-02, -3.3781e-02]], requires_grad=True)\n",
      "Gradient for Attn Value Linear Layer\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Zero Grad로 Gradient를 지웠으므로 이제 gradient는 None입니다.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient for Attn Value Linear Layer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mself\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mself\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# 첫번째 layer의 attention value linear layer의 weight를 확인해보겠습니다.\n",
    "print(\"Weight for Attn Value Linear Layer\") \n",
    "print(f\"Shape : {model.base_model.encoder.layer[0].attention.self.value.weight.shape}\")\n",
    "print(model.base_model.encoder.layer[0].attention.self.value.weight)\n",
    "\n",
    "# Zero Grad로 Gradient를 지웠으므로 이제 gradient는 None입니다.\n",
    "print(\"Gradient for Attn Value Linear Layer\")\n",
    "print(f\"Shape : {model.base_model.encoder.layer[0].attention.self.value.weight.grad.shape}\")\n",
    "print(model.base_model.encoder.layer[0].attention.self.value.weight.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Overall Train Pipeline\n",
    "## HuggingFace를 적절히 활용하면 아래처럼 학습 코드를 단순화할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "torch.cuda.set_per_process_memory_fraction(11/24)\n",
    "\n",
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaehee_kim\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jaehee/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb 사이트에서 authorization key를 발급받아 입력합니다. \n",
    "# script로 작업 시에는 명령어 창에서 입력하면 되어서 별도 login이 필요하지 않습니다. \n",
    "wandb.login(key='bfe6b67a5bdd260c5771d108854328d7e7698267')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 05:55:03.669962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\", add_pooling_layer=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "classification_head = torch.nn.Linear(768, 2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset('imdb')\n",
    "train_valid = imdb['train']\n",
    "test = imdb['test']\n",
    "# split train/validation\n",
    "train_valid = train_valid.train_test_split(test_size=0.1)\n",
    "train = train_valid['train']\n",
    "valid = train_valid['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.to_dict()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = {}\n",
    "        input_data['input_ids'] = self.data['input_ids'][idx]\n",
    "        input_data['token_type_ids'] = self.data['token_type_ids'][idx]\n",
    "        input_data['attention_mask'] = self.data['attention_mask'][idx]\n",
    "        input_data['label'] = self.data['label'][idx]\n",
    "        return input_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    data_dict = {\"input_ids\" : [], \"token_type_ids\" : [], \"attention_mask\" : [], \"label\" : []}\n",
    "    for data in batch : \n",
    "        data_dict['input_ids'].append(data['input_ids'])\n",
    "        data_dict['token_type_ids'].append(data['token_type_ids'])\n",
    "        data_dict['attention_mask'].append(data['attention_mask'])\n",
    "        data_dict['label'].append(data['label'])\n",
    "    data_dict['input_ids'] = torch.tensor(data_dict['input_ids'])\n",
    "    data_dict['token_type_ids'] = torch.tensor(data_dict['token_type_ids'])\n",
    "    data_dict['attention_mask'] = torch.tensor(data_dict['attention_mask'])\n",
    "    data_dict['label'] = torch.tensor(data_dict['label'])\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004041910171508789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 22500,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8786bcd967d9443b969bdd1d55073593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012114286422729492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 2500,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1928f11af15408da15f1e6e419be422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. dataset tokenization\n",
    "tokenized_train = train.map(lambda example: tokenizer(example['text'], truncation=True, padding='max_length'))\n",
    "tokenized_valid = valid.map(lambda example: tokenizer(example['text'], truncation=True, padding='max_length'))\n",
    "tokenized_test = test.map(lambda example: tokenizer(example['text'], truncation=True, padding='max_length'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. dataloader\n",
    "train_dataloader = DataLoader(IMDBDataset(tokenized_train), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(IMDBDataset(tokenized_valid), batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(IMDBDataset(tokenized_test), batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['token_type_ids'].shape)\n",
    "    print(batch['attention_mask'].shape)\n",
    "    print(batch['label'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : While movie titles contains the word 'Mother', the first thing that comes to our mind will be a mother's love for her children.<br /><br />However, The Mother tells a different story.<br /><br />The Mother do not discuss the love between a mother and her child, or how she sacrifice herself for the benefit of her child. Here, Notting Hill director Roger Michell tells us how a mother's love for a man about half of her age hurts the people around her.<br /><br />Before Daniel Craig takes on the role of James Bond, here, he plays Darren, a man who is helping to renovate the house of the son of the mother, and sleeping with her daughter as well. Anne Reid, who was a familiar face on TV series, takes up the challenging role of the leading character, May.<br /><br />The story begins with May coping with the sudden loss of her husband, Toots, in a family visit to her son, Bobby. While she befriends Darren, a handyman who is doing some renovation in Bobby's house, she was shocked to found out that her daughter, Paula, was sleeping with Darren. At the same time, May was coping with life after the death of Toots. Fearing that Harry and Paula do not wanted her, May starts to find her life going off track, until she spends her afternoon with Darren.<br /><br />Darren was nice and friendly to May, and May soon finds some affection on Darren. Instead of treating him like a friend, she treated the man who was about half her age with love of a couple. Later, May found sexual pleasure from Darren, where he gave her the pleasure she could never find on anyone else. And this is the beginning of the disaster that could lead to the break down of a family.<br /><br />The Mother explores the inner world of a widow who wanted to try something she never had in her life, and solace on someone who is there for her to shoulder on. This can be told from May buying tea time snacks for Darren to fulfilling sexual needs from a man younger than her, where it eventually gave her more than she bargained for.<br /><br />Anne Reid has made a breakthrough for her role of May, as she was previously best well known for her various role on TV series. As she do not have much movies in her career resume, The Mother has put her on the critic's attention. Daniel Craig, on the other hand, had took on a similar role in his movie career, such as Sylvia (2003) and Enduring Love (2004). If his reprising role of James Bond fails, film reviewers should not forget that he has a better performance in small productions in his years of movie career, and The Mother is one of them.<br /><br />The Mother may not be everyone's favorite, but it is definitely not your usual matinée show to go along with tea and scones, accompanied by butter and jam.\n",
      "label : 1\n",
      "input_ids : [101, 2096, 3185, 4486, 3397, 1996, 2773, 1005, 2388, 1005, 1010, 1996, 2034, 2518, 2008, 3310, 2000, 2256, 2568, 2097, 2022, 1037, 2388, 1005, 1055, 2293, 2005, 2014, 2336, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2174, 1010, 1996, 2388, 4136, 1037, 2367, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2388, 2079, 2025, 6848, 1996, 2293, 2090, 1037, 2388, 1998, 2014, 2775, 1010, 2030, 2129, 2016, 8688, 2841, 2005, 1996, 5770, 1997, 2014, 2775, 1012, 2182, 1010, 2025, 3436, 2940, 2472, 5074, 8709, 2140, 4136, 2149, 2129, 1037, 2388, 1005, 1055, 2293, 2005, 1037, 2158, 2055, 2431, 1997, 2014, 2287, 13403, 1996, 2111, 2105, 2014, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2077, 3817, 7010, 3138, 2006, 1996, 2535, 1997, 2508, 5416, 1010, 2182, 1010, 2002, 3248, 12270, 1010, 1037, 2158, 2040, 2003, 5094, 2000, 17738, 16952, 1996, 2160, 1997, 1996, 2365, 1997, 1996, 2388, 1010, 1998, 5777, 2007, 2014, 2684, 2004, 2092, 1012, 4776, 9027, 1010, 2040, 2001, 1037, 5220, 2227, 2006, 2694, 2186, 1010, 3138, 2039, 1996, 10368, 2535, 1997, 1996, 2877, 2839, 1010, 2089, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2466, 4269, 2007, 2089, 27520, 2007, 1996, 5573, 3279, 1997, 2014, 3129, 1010, 2205, 3215, 1010, 1999, 1037, 2155, 3942, 2000, 2014, 2365, 1010, 6173, 1012, 2096, 2016, 2022, 19699, 9013, 5104, 12270, 1010, 1037, 18801, 2386, 2040, 2003, 2725, 2070, 10525, 1999, 6173, 1005, 1055, 2160, 1010, 2016, 2001, 7135, 2000, 2179, 2041, 2008, 2014, 2684, 1010, 13723, 1010, 2001, 5777, 2007, 12270, 1012, 2012, 1996, 2168, 2051, 1010, 2089, 2001, 27520, 2007, 2166, 2044, 1996, 2331, 1997, 2205, 3215, 1012, 14892, 2008, 4302, 1998, 13723, 2079, 2025, 2359, 2014, 1010, 2089, 4627, 2000, 2424, 2014, 2166, 2183, 2125, 2650, 1010, 2127, 2016, 15970, 2014, 5027, 2007, 12270, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 12270, 2001, 3835, 1998, 5379, 2000, 2089, 1010, 1998, 2089, 2574, 4858, 2070, 12242, 2006, 12270, 1012, 2612, 1997, 12318, 2032, 2066, 1037, 2767, 1010, 2016, 5845, 1996, 2158, 2040, 2001, 2055, 2431, 2014, 2287, 2007, 2293, 1997, 1037, 3232, 1012, 2101, 1010, 2089, 2179, 4424, 5165, 2013, 12270, 1010, 2073, 2002, 2435, 2014, 1996, 5165, 2016, 2071, 2196, 2424, 2006, 3087, 2842, 1012, 1998, 2023, 2003, 1996, 2927, 1997, 1996, 7071, 2008, 2071, 2599, 2000, 1996, 3338, 2091, 1997, 1037, 2155, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2388, 15102, 1996, 5110, 2088, 1997, 1037, 7794, 2040, 2359, 2000, 3046, 2242, 2016, 2196, 2018, 1999, 2014, 2166, 1010, 1998, 14017, 10732, 2006, 2619, 2040, 2003, 2045, 2005, 2014, 2000, 3244, 2006, 1012, 2023, 2064, 2022, 2409, 2013, 2089, 9343, 5572, 2051, 27962, 2005, 12270, 2000, 21570, 4424, 3791, 2013, 1037, 2158, 3920, 2084, 2014, 1010, 2073, 2009, 2776, 2435, 2014, 2062, 2084, 2016, 17113, 2098, 2005, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 4776, 9027, 2038, 2081, 1037, 12687, 2005, 2014, 2535, 1997, 2089, 1010, 2004, 2016, 2001, 3130, 2190, 2092, 2124, 2005, 2014, 2536, 2535, 2006, 2694, 2186, 1012, 2004, 2016, 102]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for key, value in tokenized_train[0].items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 3\n",
    "step = 0\n",
    "model.to('cuda')\n",
    "classification_head.to('cuda')\n",
    "\n",
    "# tqdm을 사용할 때 전체 step 수를 계산하기 위해 EPOCH * len(train_dataloader)를 사용합니다.\n",
    "counter = tqdm(range(EPOCH*len(train_dataloader)), desc=\"Training :\")\n",
    "# wandb에서 실험 내용을 확인할 수 있도록 wandb init을 수행합니다.\n",
    "# 이때 실험에 사용된 다양한 hyperparameter들을 config로 함께 기록합니다.\n",
    "wandb.init(\n",
    "    project=\"dsba_pretrain_nlp_exp1\", \n",
    "    name='[imdb] bert-base-uncased',\n",
    "    config={\n",
    "        \"model\": \"bert-base-uncased\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": 1e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"epoch\": EPOCH,\n",
    "        \"max_seq_length\": 512\n",
    "    })\n",
    "\n",
    "logging_dir = \"log\"\n",
    "if os.path.exists(logging_dir) == False:\n",
    "    os.makedirs(logging_dir)\n",
    "logging.basicConfig(\n",
    "    filename=f\"{logging_dir}/[imdb] bert-base-uncased-train.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for batch in train_dataloader:\n",
    "        # 학습에 사용할 데이터를 cuda로 옮깁니다.\n",
    "        label = batch['label'].to('cuda')\n",
    "        model_input = {key: value.to('cuda') for key, value in batch.items() if key != 'label'}\n",
    "        # 이전 step에서 계산된 gradient를 지웁니다.\n",
    "        optimizer.zero_grad()\n",
    "        # pretrained lm을 통과시켜 representation을 산출합니다.\n",
    "        representation = model(**model_input)\n",
    "        # representation을 이용해 classification을 수행합니다.\n",
    "        pooled_output = torch.mean(representation['last_hidden_state'], dim=1)\n",
    "        logits = classification_head(pooled_output)\n",
    "        # loss를 계산하고, 역전파를 수행합니다.\n",
    "        train_loss = loss_fn(logits, label)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        counter.update(1)\n",
    "        # wandb에 loss를 기록합니다.\n",
    "        wandb.log({\"loss\": train_loss})\n",
    "        if step % 100 == 0:\n",
    "            print(f\"step : {step}, loss : {train_loss}\")\n",
    "            logging.info(f\"step : {step}, loss : {train_loss}\")\n",
    "\n",
    "        # validation set을 이용해 모델의 성능을 확인합니다.\n",
    "        if step % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                classification_head.eval()\n",
    "                valid_loss = 0\n",
    "                valid_step = 0\n",
    "                valid_acc = []\n",
    "                for batch in valid_dataloader:\n",
    "                    label = batch['label'].to('cuda')\n",
    "                    model_input = {key: value.to('cuda') for key, value in batch.items() if key != 'label'}\n",
    "                    representation = model(**model_input)\n",
    "                    pooled_output = torch.mean(representation['last_hidden_state'], dim=1)\n",
    "                    logits = classification_head(pooled_output)\n",
    "                    loss = loss_fn(logits, label)\n",
    "                    valid_loss += loss.item()\n",
    "                    valid_step += 1\n",
    "                    valid_acc.append(torch.argmax(logits, dim=-1) == label)\n",
    "                print(f\"valid loss : {valid_loss/valid_step}\")\n",
    "                print(f\"valid acc : {torch.cat(valid_acc, dim=0).float().mean()}\")\n",
    "                # wandb에 valid loss와 valid accuracy를 기록합니다.\n",
    "                wandb.log({\"valid_loss\": valid_loss/valid_step, \"valid_acc\": torch.cat(valid_acc, dim=0).float().mean()})\n",
    "                logger.info(f\"valid loss : {valid_loss/valid_step}\")\n",
    "                model.train()\n",
    "                classification_head.train()\n",
    "\n",
    "        if step % 1000 == 0: # 1000 step마다 모델을 저장합니다.\n",
    "            if os.path.exists(f\"log/model_log/{step}\") == False:\n",
    "                os.makedirs(f\"log/model_log/{step}\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"log/model_log/{step}/bert_model_{step}.pt\")\n",
    "            torch.save(classification_head.state_dict(), f\"log/model_log/{step}/bert_classification_head_{step}.pt\")\n",
    "\n",
    "# 모델 학습이 끝나면 test set을 이용해 모델의 성능을 확인합니다.\n",
    "with torch.no_grad() :\n",
    "    model.eval()\n",
    "    classification_head.eval()\n",
    "    test_loss = 0\n",
    "    test_step = 0\n",
    "    test_acc = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        label = batch['label'].to('cuda')\n",
    "        model_input = {key: value.to('cuda') for key, value in batch.items() if key != 'label'}\n",
    "        representation = model(**model_input)\n",
    "        pooled_output = torch.mean(representation['last_hidden_state'], dim=1)\n",
    "        logits = classification_head(pooled_output)\n",
    "        loss = loss_fn(logits, label)\n",
    "        test_loss += loss\n",
    "        test_step += 1\n",
    "        test_acc.append(torch.argmax(logits, dim=-1) == label)\n",
    "\n",
    "print(f\"test loss : {test_loss/test_step}\")\n",
    "print(f\"test acc : {torch.cat(test_acc, dim=0).float().mean()}\")\n",
    "wandb.log({\"test_loss\": test_loss/test_step, \"test_acc\": torch.cat(test_acc, dim=0).float().mean()})\n",
    "logger.info(f\"test loss : {test_loss/test_step}\")\n",
    "\n",
    "if step % 1000 == 0: # 1000 step마다 모델을 저장합니다.\n",
    "    if os.path.exists(f\"log/model_log/{step}\") == False:\n",
    "        os.makedirs(f\"log/model_log/{step}\", exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"log/model_log/{step}/bert_model_last.pt\")\n",
    "torch.save(classification_head.state_dict(), f\"log/model_log/{step}/bert_classification_head_last.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
